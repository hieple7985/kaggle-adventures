{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":66631,"databundleVersionId":8346466,"sourceType":"competition"}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.vec_env import DummyVecEnv\nfrom stable_baselines3.common.monitor import Monitor\nfrom stable_baselines3.common.callbacks import EvalCallback\nfrom stable_baselines3.common.callbacks import StopTrainingOnRewardThreshold\nfrom stable_baselines3.common.utils import get_linear_fn\n\n# Check for GPU\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Load datasets\ntrain = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')\ntest = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')\nsample_submission = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv')\n\n# TF-IDF Vectorization\nvectorizer = TfidfVectorizer(max_features=1000)\ntrain_tfidf = vectorizer.fit_transform(train['response_a'] + \" \" + train['response_b'])\ntest_tfidf = vectorizer.transform(test['response_a'] + \" \" + test['response_b'])\n\n# Additional Features\ntrain['response_a_length'] = train['response_a'].apply(len)\ntrain['response_b_length'] = train['response_b'].apply(len)\ntrain['response_a_word_count'] = train['response_a'].apply(lambda x: len(x.split()))\ntrain['response_b_word_count'] = train['response_b'].apply(lambda x: len(x.split()))\n\ntest['response_a_length'] = test['response_a'].apply(len)\ntest['response_b_length'] = test['response_b'].apply(len)\ntest['response_a_word_count'] = test['response_a'].apply(lambda x: len(x.split()))\ntest['response_b_word_count'] = test['response_b'].apply(lambda x: len(x.split()))\n\n# Combine Features\nX_train_combined = np.hstack((train_tfidf.toarray(), train[['response_a_length', 'response_b_length', 'response_a_word_count', 'response_b_word_count']].values))\nX_test_combined = np.hstack((test_tfidf.toarray(), test[['response_a_length', 'response_b_length', 'response_a_word_count', 'response_b_word_count']].values))\n\ny_a = train['winner_model_a'].values\ny_b = train['winner_model_b'].values\ny_tie = train['winner_tie'].values\n\n# Train a simple logistic regression as reward model\nreward_model = LogisticRegression()\nreward_model.fit(X_train_combined, y_a)\n\n# Define custom environment for PPO\nimport gymnasium as gym\nfrom gymnasium import spaces\n\nclass ChatbotEnv(gym.Env):\n    def __init__(self, data, reward_model):\n        super(ChatbotEnv, self).__init__()\n        self.data = data\n        self.reward_model = reward_model\n        self.current_step = 0\n        self.action_space = spaces.Discrete(2)  # Two actions: A or B\n        self.observation_space = spaces.Box(low=0, high=1, shape=(data.shape[1],), dtype=np.float32)\n\n    def reset(self):\n        self.current_step = 0\n        return self.data[self.current_step], {}\n\n    def step(self, action):\n        obs = self.data[self.current_step]\n        reward = self.reward_model.predict_proba([obs])[0][action]\n        self.current_step += 1\n        done = self.current_step >= len(self.data)\n        next_obs = self.data[self.current_step] if not done else self.data[0]\n        return next_obs, reward, done, {}, {}  # Return observation, reward, done, truncated, info\n\n# Initialize environment with smaller subset size and wrap it with Monitor and DummyVecEnv\nenv = ChatbotEnv(X_train_combined[:1000], reward_model)  # Start with 1k samples\nenv = Monitor(env)\nenv = DummyVecEnv([lambda: env])\n\n# Initialize PPO model\nmodel = PPO(\"MlpPolicy\", env, verbose=1, batch_size=32, learning_rate=1e-3, n_steps=512, clip_range=0.2)\n\n# Define early stopping callback\ncallback_on_best = StopTrainingOnRewardThreshold(reward_threshold=20000, verbose=1)\neval_callback = EvalCallback(env, best_model_save_path='./logs/',\n                             log_path='./logs/', eval_freq=1000,\n                             deterministic=True, render=False,\n                             callback_on_new_best=callback_on_best)\n\n# Training PPO model with fewer timesteps initially\nmodel.learn(total_timesteps=1000, callback=eval_callback)\n\n# Gradually increase the data subset and timesteps without changing the environment\nfor subset_size in [2000, 4000, 8000, len(X_train_combined)]:\n    env.data = X_train_combined[:subset_size]\n    model.learn(total_timesteps=1000, callback=eval_callback)\n\n# Generate predictions using the trained PPO model\nobs, _ = env.reset()\ndone = False\nresponses = []\nwhile not done:\n    action, _states = model.predict(obs)\n    obs, reward, done, _, info = env.step(action)  # updated to handle 4 return values\n    responses.append(action)\n\n# Convert responses to numpy array\nresponses = np.array(responses).flatten()\n\n# Prepare the submission file with PPO model predictions\nsubmission = pd.DataFrame({\n    'id': test['id'],\n    'winner_model_a': responses[:len(test)],\n    'winner_model_b': responses[:len(test)],\n    'winner_tie': responses[:len(test)]\n})\n\n# Save the submission file\nsubmission.to_csv('submission.csv', index=False)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-04T13:58:13.411502Z","iopub.execute_input":"2024-08-04T13:58:13.412249Z","iopub.status.idle":"2024-08-04T13:58:39.330257Z","shell.execute_reply.started":"2024-08-04T13:58:13.412204Z","shell.execute_reply":"2024-08-04T13:58:39.328685Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Using cpu device\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[2], line 93\u001b[0m\n\u001b[1;32m     87\u001b[0m eval_callback \u001b[38;5;241m=\u001b[39m EvalCallback(env, best_model_save_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./logs/\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     88\u001b[0m                              log_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./logs/\u001b[39m\u001b[38;5;124m'\u001b[39m, eval_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m,\n\u001b[1;32m     89\u001b[0m                              deterministic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, render\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     90\u001b[0m                              callback_on_new_best\u001b[38;5;241m=\u001b[39mcallback_on_best)\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# Training PPO model with fewer timesteps initially\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_callback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# Gradually increase the data subset and timesteps without changing the environment\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m subset_size \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m2000\u001b[39m, \u001b[38;5;241m4000\u001b[39m, \u001b[38;5;241m8000\u001b[39m, \u001b[38;5;28mlen\u001b[39m(X_train_combined)]:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py:308\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[1;32m    301\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    306\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    307\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 308\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:246\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfOnPolicyAlgorithm,\n\u001b[1;32m    237\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    242\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    243\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfOnPolicyAlgorithm:\n\u001b[1;32m    244\u001b[0m     iteration \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 246\u001b[0m     total_timesteps, callback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_learn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m     callback\u001b[38;5;241m.\u001b[39mon_training_start(\u001b[38;5;28mlocals\u001b[39m(), \u001b[38;5;28mglobals\u001b[39m())\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/stable_baselines3/common/base_class.py:424\u001b[0m, in \u001b[0;36mBaseAlgorithm._setup_learn\u001b[0;34m(self, total_timesteps, callback, reset_num_timesteps, tb_log_name, progress_bar)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;66;03m# pytype: disable=annotation-type-mismatch\u001b[39;00m\n\u001b[0;32m--> 424\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;66;03m# pytype: enable=annotation-type-mismatch\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_episode_starts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mnum_envs,), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:76\u001b[0m, in \u001b[0;36mDummyVecEnv.reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvObs:\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[0;32m---> 76\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_seeds\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_obs(env_idx, obs)\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;66;03m# Seeds are only used once\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/stable_baselines3/common/monitor.py:83\u001b[0m, in \u001b[0;36mMonitor.reset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected you to pass keyword argument \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m into reset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_reset_info[key] \u001b[38;5;241m=\u001b[39m value\n\u001b[0;32m---> 83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mTypeError\u001b[0m: ChatbotEnv.reset() got an unexpected keyword argument 'seed'"],"ename":"TypeError","evalue":"ChatbotEnv.reset() got an unexpected keyword argument 'seed'","output_type":"error"}]}]}