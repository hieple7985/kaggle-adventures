{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":63056,"databundleVersionId":9094797,"sourceType":"competition"}],"dockerImageVersionId":30747,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Revision\n\n- v1: initital\n- v2: clean up code\n\n# Todos\n\n# To-Do List for Achieving Golden Score\n\n## 1. **Review and Clean Up the Notebook**\n   - [x] **Remove Deprecated Code:**\n     - Identify and remove commented-out code that is no longer in use.\n     - Ensure that any relevant documentation or explanations are retained.\n   - [ ] **Refactor Code:**\n     - Simplify any complex logic where possible.\n     - Ensure the code is clean, concise, and well-commented.\n\n## 2. **Enhance Data Preprocessing**\n   - [ ] **Feature Engineering:**\n     - Explore additional feature engineering techniques to extract more valuable information.\n     - Consider domain-specific features that may enhance model performance.\n   - [ ] **Data Augmentation:**\n     - Implement data augmentation techniques to increase the diversity of the training set, particularly for positive samples.\n     - Evaluate the impact of augmentation on model performance.\n\n## 3. **Model Tuning and Selection**\n   - [ ] **Hyperparameter Optimization:**\n     - Revisit hyperparameter tuning using Optuna or other optimization frameworks.\n     - Consider increasing the number of trials to explore a broader search space.\n   - [ ] **Model Ensemble:**\n     - Experiment with different ensemble techniques, such as stacking or blending, to combine the strengths of multiple models.\n     - Optimize the weights for the model fusion to maximize performance.\n   - [ ] **Algorithm Exploration:**\n     - Explore additional algorithms (e.g., Neural Networks) that might outperform current models.\n     - Evaluate these models in the same GroupKFold cross-validation setup.\n\n## 4. **Secondary Metrics Optimization**\n   - [ ] **Top-15 Retrieval Sensitivity:**\n     - Implement and test the secondary prize metric as described.\n     - Optimize the model specifically for the Top-15 retrieval sensitivity metric to increase chances of winning the secondary prize.\n   - [ ] **Model Efficiency:**\n     - Evaluate and optimize the efficiency of the model by balancing runtime and accuracy.\n     - Explore potential model simplifications or adjustments to improve runtime without significantly sacrificing performance.\n\n## 5. **Feature Selection and Reduction**\n   - [ ] **Correlation Analysis:**\n     - Perform a thorough correlation analysis to identify and remove highly correlated features.\n     - Consider techniques like PCA or other dimensionality reduction methods if feature space is large.\n   - [ ] **Feature Importance:**\n     - Use feature importance scores from models like XGBoost or LGBM to select the most impactful features.\n     - Remove low-importance features to reduce overfitting and improve generalization.\n\n## 6. **Cross-Validation Strategy**\n   - [ ] **GroupKFold Tuning:**\n     - Fine-tune the GroupKFold strategy to ensure that patient data is properly stratified and leakage is minimized.\n     - Explore alternative cross-validation techniques if necessary.\n\n## 7. **Final Model Evaluation**\n   - [ ] **Out-of-Fold Predictions:**\n     - Use out-of-fold predictions to evaluate the final model's performance on unseen data.\n   - [ ] **Private Leaderboard Testing:**\n     - Run the model on a holdout set or using a simulated private leaderboard setup to validate the final performance before submission.\n   - [ ] **Submission Preparation:**\n     - Ensure that the final submission meets all competition guidelines and requirements.\n\n## 8. **Documentation and Reporting**\n   - [ ] **Notebook Documentation:**\n     - Update the notebook with clear explanations of each step taken.\n     - Include reasoning for choices made during the modeling process.\n   - [ ] **Result Analysis:**\n     - Document the results of each model iteration, including any improvements or declines in performance.\n   - [ ] **Submission Commentary:**\n     - Prepare a submission report that explains the model’s methodology, strengths, and any unique approaches taken.\n\n## 9. **Explore External Data**\n   - [ ] **External Dataset Integration:**\n     - Research and incorporate external datasets if allowed by the competition rules.\n     - Ensure that the external data is preprocessed consistently with the internal dataset.\n\n## 10. **Community Engagement**\n   - [ ] **Kaggle Discussions:**\n     - Engage with the Kaggle community to share insights and gain feedback on approaches.\n   - [ ] **Review Competitor Notebooks:**\n     - Analyze and learn from top competitors' public notebooks to gain new ideas and insights.\n\n## 11. **Final Submission and Backup Plan**\n   - [ ] **Backup Model:**\n     - Prepare a secondary model or approach as a backup in case the primary model underperforms on the private leaderboard.\n   - [ ] **Final Submission:**\n     - Ensure that the final model is thoroughly tested, validated, and ready for submission.\n     - Double-check submission format and file integrity.\n\n# Steps\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"## Libraries and Config","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom  lightgbm import LGBMClassifier,log_evaluation\nfrom catboost import CatBoostClassifier\nfrom xgboost import XGBClassifier\nfrom tqdm import tqdm\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import GroupKFold\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\nclass Config():\n    seed=2024\n    num_folds=10\n    TARGET_NAME ='target'\n    \nimport random\n    \ndef seed_everything(seed):\n    np.random.seed(seed)\n    random.seed(seed)\nseed_everything(Config.seed)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Read Dataset\n\nWe can see that the ratio of targets is 1000:1","metadata":{}},{"cell_type":"code","source":"train=pd.read_csv(\"/kaggle/input/isic-2024-challenge/train-metadata.csv\")\nprint(f\"len(train):{len(train)}\")\ntest=pd.read_csv(\"/kaggle/input/isic-2024-challenge/test-metadata.csv\")\nprint(f\"len(test):{len(test)}\")\n\n# Saving GPU time offline\n# if len(test) == 3:\n#     train=train[:50000]\n\ntrain.head()\ntrain[Config.TARGET_NAME].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## EDA Analysis Results for Tabular Data\n\n- **`isic_id`**: Unique identifier.\n- **`patient_id`**: There are 1,042 unique patients.\n- **`target`**: This is the label (binary outcome).\n\n### Demographics\n- **`sex`**: Males have a slightly higher positive rate than females, approximately 5:4.\n- **`age_approx`**: No cases were found below the age of 18. The `age_approx` variable has a high Pearson correlation coefficient with `target.mean()`.\n\n### Anatomical Sites\n- **`anatom_site_general`**: The `Head/Neck` region has 64 cases, while other categories have around 8 cases each.\n- **`clin_size_long_diam_mm`**: The count shows a long-tailed distribution. In some cases, `target.mean()` is 0, which might be due to small sample size; subjective conclusions should not be drawn.\n\n### Image and Lesion Types\n- **`image_type`**: Only one category is present, making it not useful.\n- **`tbp_tile_type`**: For `3D:XP` and `3D:white`, `target.mean()` is 6 and 17, respectively.\n\n### Lesion Volume Features\n- **`tbp_lv_A`, `tbp_lv_Aext`, `tbp_lv_B`, `tbp_lv_Bext`, `tbp_lv_C`, `tbp_lv_Cext`, `tbp_lv_H`, `tbp_lv_Hext`, `tbp_lv_L`, `tbp_lv_Lext`**: These continuous variables follow a normal distribution.\n- **`tbp_lv_areaMM2`**: The data shows a long-tailed distribution.\n- **`tbp_lv_area_perim_ratio`**: The data shows a long-tailed distribution.\n- **`tbp_lv_color_std_mean`**: Extremely long-tailed distribution; consider categorizing into two classes based on whether the value is 0 or not.\n- **`tbp_lv_deltaA`, `tbp_lv_deltaB`, `tbp_lv_deltaL`**: Continuous variables, normally distributed.\n- **`tbp_lv_deltaLB`**: Continuous variable, normally distributed. Values ≥ 25 might be outliers.\n- **`tbp_lv_deltaLBnorm`**: Continuous variable, normally distributed. Values ≥ 20 might be outliers.\n- **`tbp_lv_eccentricity`**: Continuous variable, normally distributed.\n- **`tbp_lv_location`**: The `Head & Neck` region seems to have the highest incidence. Some other categories have `target.mean()` equal to 0; it is unclear if this is due to insufficient data or if it can be used as a conclusion.\n- **`tbp_lv_location_simple`**: Categories might overlap with those in `tbp_lv_location`.\n- **`tbp_lv_minorAxisMM`**: Shows a long-tailed distribution.\n- **`tbp_lv_nevi_confidence`**: Right-skewed long-tailed distribution.\n- **`tbp_lv_norm_border`**: Right-skewed long-tailed distribution.\n- **`tbp_lv_norm_color`**: Significant clustering at values 10 and 0; consider categorizing into three classes: 0, 10, and others.\n- **`tbp_lv_perimeterMM`**: Long-tailed distribution.\n- **`tbp_lv_radial_color_std_max`**: Most values are 0, with a long-tailed distribution.\n- **`tbp_lv_stdL`, `tbp_lv_stdLExt`**: Somewhat long-tailed distributions.\n- **`tbp_lv_symm_2axis`**: Continuous variable, with particularly high counts at a few points.\n- **`tbp_lv_symm_2axis_angle`**: Categorical variable.\n- **`tbp_lv_x`, `tbp_lv_y`, `tbp_lv_z`**: Normally distributed continuous variables, with `tbp_lv_y` being left-skewed.\n\n### Metadata\n- **`attribution`**: Categorical variable.\n- **`copyright_license`**: The mean value for `CC-BY-NC` is particularly low.\n\n### Lesion and Diagnosis Data\n- **`lesion_id`**: There are 20,000 lesion IDs with 400,000 training data points.\n- **`iddx_full`**: Over 390,000 belong to one category (`Benign`). Consider whether the remaining categories should be further subdivided.\n- **`iddx_1`**: A categorical variable with three categories.\n- **`iddx_2`, `iddx_3`, `iddx_4`**: Variables with missing and non-missing values; consider combining them into one category.\n- **`iddx_5`**: Only one value is present; the rest are missing, so it should be dropped.\n- **`mel_mitotic_index`**: A few values are present; it is a categorical variable with an ordinal relationship.\n- **`mel_thick_mm`**: Combine cases with and without values into one category, as only a few have values.\n- **`tbp_lv_dnn_lesion_confidence`**: Continuous variable with a right-skewed long-tailed distribution.\n","metadata":{}},{"cell_type":"markdown","source":"## Feature Engineering\n\nPart of the features were created by myself, and part came from <a href=\"https://www.kaggle.com/code/abdmental01/multimodel-isic/notebook\">multimodel-isic</a>.","metadata":{}},{"cell_type":"code","source":"#训练数据里的类别型变量\nprint(\"tbp_lv_dnn_lesion_confidence feature\")\ncates=['age_approx', 'sex', 'anatom_site_general', 'tbp_tile_type', 'tbp_lv_location_simple', 'attribution', 'copyright_license']\n#'tbp_lv_dnn_lesion_confidence'构造groupby特征\ncates2mean={}\nfor c in cates:\n    base=train.groupby(c)['tbp_lv_dnn_lesion_confidence'].mean().reset_index().rename(columns={'tbp_lv_dnn_lesion_confidence':f'mean_{c}_tbp_lv_dnn_lesion_confidence'})\n    \n    min_tmp=train.groupby(c)['tbp_lv_dnn_lesion_confidence'].min().reset_index().rename(columns={'tbp_lv_dnn_lesion_confidence':f'min_{c}_tbp_lv_dnn_lesion_confidence'})\n    base=base.merge(min_tmp,on=c,how='left')\n    \n    max_tmp=train.groupby(c)['tbp_lv_dnn_lesion_confidence'].max().reset_index().rename(columns={'tbp_lv_dnn_lesion_confidence':f'max_{c}_tbp_lv_dnn_lesion_confidence'})\n    base=base.merge(max_tmp,on=c,how='left')\n    \n    median_tmp=train.groupby(c)['tbp_lv_dnn_lesion_confidence'].median().reset_index().rename(columns={'tbp_lv_dnn_lesion_confidence':f'median_{c}_tbp_lv_dnn_lesion_confidence'})\n    base=base.merge(median_tmp,on=c,how='left')\n    \n    std_tmp=train.groupby(c)['tbp_lv_dnn_lesion_confidence'].std().reset_index().rename(columns={'tbp_lv_dnn_lesion_confidence':f'std_{c}_tbp_lv_dnn_lesion_confidence'})\n    base=base.merge(std_tmp,on=c,how='left')\n    \n    skew_tmp=train.groupby(c)['tbp_lv_dnn_lesion_confidence'].skew().reset_index().rename(columns={'tbp_lv_dnn_lesion_confidence':f'skew_{c}_tbp_lv_dnn_lesion_confidence'})\n    base=base.merge(skew_tmp,on=c,how='left')\n    \n    cates2mean[c]=base\n\ndef FE(df):\n    #特征工程出处:https://www.kaggle.com/code/abdmental01/multimodel-isic\n    #这部分特征工程可能需要一些专业的背景知识,这里我也没有深究 \n    df[\"lesion_size_ratio\"]=df[\"tbp_lv_minorAxisMM\"]/df[\"clin_size_long_diam_mm\"]\n    df[\"lesion_shape_index\"]=df[\"tbp_lv_areaMM2\"]/(df[\"tbp_lv_perimeterMM\"]**2)\n    df[\"hue_contrast\"]= (df[\"tbp_lv_H\"]-df[\"tbp_lv_Hext\"]).abs()\n    df[\"luminance_contrast\"]= (df[\"tbp_lv_L\"]-df[\"tbp_lv_Lext\"]).abs()\n    df[\"lesion_color_difference\"]=np.sqrt(df[\"tbp_lv_deltaA\"]**2+df[\"tbp_lv_deltaB\"]**2+df[\"tbp_lv_deltaL\"]**2)\n    df[\"border_complexity\"]=df[\"tbp_lv_norm_border\"]+df[\"tbp_lv_symm_2axis\"]\n    df[\"3d_position_distance\"]=np.sqrt(df[\"tbp_lv_x\"]**2+df[\"tbp_lv_y\"]**2+df[\"tbp_lv_z\"]**2)\n    df[\"perimeter_to_area_ratio\"]=df[\"tbp_lv_perimeterMM\"]/df[\"tbp_lv_areaMM2\"]\n    df[\"lesion_visibility_score\"]=df[\"tbp_lv_deltaLBnorm\"]+df[\"tbp_lv_norm_color\"]\n    df[\"combined_anatomical_site\"]=df[\"anatom_site_general\"]+\"_\"+df[\"tbp_lv_location\"]\n    df[\"symmetry_border_consistency\"]=df[\"tbp_lv_symm_2axis\"]*df[\"tbp_lv_norm_border\"]\n    df[\"color_consistency\"]=df[\"tbp_lv_stdL\"]/df[\"tbp_lv_Lext\"]\n    df[\"size_age_interaction\"]=df[\"clin_size_long_diam_mm\"]*df[\"age_approx\"]\n    df[\"hue_color_std_interaction\"]=df[\"tbp_lv_H\"]*df[\"tbp_lv_color_std_mean\"]\n    df[\"lesion_severity_index\"]=(df[\"tbp_lv_norm_border\"]+df[\"tbp_lv_norm_color\"]+df[\"tbp_lv_eccentricity\"])/3\n    df[\"shape_complexity_index\"]=df[\"border_complexity\"]+df[\"lesion_shape_index\"]\n    df[\"color_contrast_index\"]=df[\"tbp_lv_deltaA\"]+df[\"tbp_lv_deltaB\"]+df[\"tbp_lv_deltaL\"]+df[\"tbp_lv_deltaLBnorm\"]\n    df[\"normalized_lesion_size\"]=df[\"clin_size_long_diam_mm\"]/df[\"age_approx\"]\n    df[\"mean_hue_difference\"]=(df[\"tbp_lv_H\"]+df[\"tbp_lv_Hext\"])/2\n    df[\"std_dev_contrast\"]=np.sqrt((df[\"tbp_lv_deltaA\"]**2+df[\"tbp_lv_deltaB\"]**2+df[\"tbp_lv_deltaL\"]**2)/3)\n    df[\"color_shape_composite_index\"]=(df[\"tbp_lv_color_std_mean\"]+df[\"tbp_lv_area_perim_ratio\"]+df[\"tbp_lv_symm_2axis\"])/3\n    df[\"3d_lesion_orientation\"]=np.arctan2(df[\"tbp_lv_y\"],df[\"tbp_lv_x\"])\n    df[\"overall_color_difference\"]=(df[\"tbp_lv_deltaA\"]+df[\"tbp_lv_deltaB\"]+df[\"tbp_lv_deltaL\"])/3\n    df[\"symmetry_perimeter_interaction\"]=df[\"tbp_lv_symm_2axis\"]*df[\"tbp_lv_perimeterMM\"]\n    df[\"comprehensive_lesion_index\"]=(df[\"tbp_lv_area_perim_ratio\"]+df[\"tbp_lv_eccentricity\"]+df[\"tbp_lv_norm_color\"]+df[\"tbp_lv_symm_2axis\"])/4\n\n    \n    print(\"drop_cols\")\n    drop_cols=['lesion_id',#训练数据有且测试数据没有,缺失值占比0.945,每个id出现1次,故drop\n     'iddx_2', #训练数据有且测试数据没有,缺失值占比0.997,故drop\n     'iddx_3', #训练数据有且测试数据没有,缺失值占比0.997,故drop\n     'iddx_4',#训练数据有且测试数据没有,缺失值占比0.998,故drop\n     'iddx_5',#训练数据有且测试数据没有,缺失值占比0.99999,故drop\n     'mel_mitotic_index',#训练数据有且测试数据没有,缺失值占比0.9998,故drop\n     'mel_thick_mm',#训练数据有且测试数据没有,缺失值占比0.9998,故drop    \n     'image_type',#训练数据中nunique=1\n     #'isic_id',#就像普通的id一样没什么意义\n     #可能本来就是知道target才有这两列数据\n     'iddx_full',#训练数据有且测试数据没有,和target有一一对应关系,每个类别target.mean()不是0就是1\n     'iddx_1',#训练数据有且测试数据没有,和target有一一对应关系,每个类别target.mean()不是0就是1 \n    ]\n    #如果测试数据没有这些列可以忽略掉\n    df.drop(drop_cols,axis=1,inplace=True,errors='ignore')\n    print(\"tbp_lv_dnn_lesion_confidence feature\")\n    for c in cates:\n        df=df.merge(cates2mean[c],on=c,how='left')\n        \n    print(\"age_approx feature\")\n    #年龄低于15岁的变成15岁\n    df.loc[df['age_approx']<=15,'age_approx']=15\n    #缺失值用最多的填充\n    df.loc[(df['age_approx']!=df['age_approx']),'age_approx']=55\n    value_counts={55.0: 58123,\n                 65.0: 54946,\n                 60.0: 54109,\n                 50.0: 47924,\n                 70.0: 39775,\n                 40.0: 31297,\n                 75.0: 30801,\n                 45.0: 23580,\n                 80.0: 21096,\n                 35.0: 11543,\n                 30.0: 10400,\n                 85.0: 8847,\n                 25.0: 3433,\n                 20.0: 1742,\n                 15.0: 645}\n    df['age_approx_count']=df['age_approx'].apply(lambda x:value_counts.get(x,645))\n    \n    \n    print(\"sex feature\")\n    #sex\n    df['sex_male']=(df['sex']=='male').astype(np.int8)\n    df['sex_female']=(df['sex']=='female').astype(np.int8)\n    value_counts={'male':265546,'female':123996}\n    #nan的value_counts\n    df['sex']=df['sex'].apply(lambda x:value_counts.get(x,11517))\n    \n    #'anatom_site_general'  one-hot\n    print(\"anatom_site_general feature\")\n    cols=['posterior torso','lower extremity','anterior torso','upper extremity','head/neck']\n    for col in cols:\n        df[f'anatom_site_general_{col}']=(df['anatom_site_general']==col).astype(np.int8)\n    #value_counts\n    value_counts={'posterior torso': 121902,\n     'lower extremity': 103028,\n     'anterior torso': 87770,\n     'upper extremity': 70557,\n     'head/neck': 12046}\n    df['anatom_site_general']=df['anatom_site_general'].apply(lambda x:value_counts.get(x,12046))\n    \n    print(\"clin_size_long_diam_mm feature\")\n    #这个长尾分布感觉修正也修正的一般\n    df['clin_size_long_diam_mm']=np.log1p(df['clin_size_long_diam_mm'])\n    \n    print(\"tbp_tile_type feature\")\n    df['tbp_tile_type']=(df['tbp_tile_type']=='3D: XP').astype(np.int8)\n    \n    print(\"tbp_lv_XX', 'tbp_lv_XXext feature\")\n    #不知道具体含义的暴力特征构造\n    for c in ['A','B','C','H','L']:\n        col1,col2=f'tbp_lv_{c}',f'tbp_lv_{c}ext'\n        df[f'{col1}+{col2}']=df[col1]+df[col2]\n        df[f'{col1}-{col2}']=df[col1]-df[col2]\n        df[f'{col1}*{col2}']=df[col1]*df[col2]\n        df[f'{col1}/{col2}']=df[col1]/(df[col2]+1e-20)  \n        \n    print(\"tbp_lv_areaMM2 feature\")\n    df['tbp_lv_areaMM2']=np.log1p(df['tbp_lv_areaMM2'])\n    \n    print(\"tbp_lv_area_perim_ratio feature\")\n    #tbp_lv_area_perim_ratio是长尾分布\n    df['tbp_lv_area_perim_ratio']=np.log1p(df['tbp_lv_area_perim_ratio'])\n    #修正大于4的异常值为均值\n    df.loc[df['tbp_lv_area_perim_ratio']>=4,'tbp_lv_area_perim_ratio']=2.9\n    \n    print(\"tbp_lv_symm_2axis feature\")\n    #tbp_lv_symm_2axis_angle应该是一个角度,故考虑sin和cos\n    df['sin_tbp_lv_symm_2axis_angle']=np.sin(2*np.pi*df['tbp_lv_symm_2axis_angle']/180)\n    df['cos_tbp_lv_symm_2axis_angle']=np.cos(2*np.pi*df['tbp_lv_symm_2axis_angle']/180)\n    df['tbp_lv_symm_2axis*sin_tbp_lv_symm_2axis_angle']=df['tbp_lv_symm_2axis']*df['sin_tbp_lv_symm_2axis_angle']\n    df['tbp_lv_symm_2axis*cos_tbp_lv_symm_2axis_angle']=df['tbp_lv_symm_2axis']*df['cos_tbp_lv_symm_2axis_angle']\n    df['tbp_lv_symm_2axis/sin_tbp_lv_symm_2axis_angle']=df['tbp_lv_symm_2axis']/df['sin_tbp_lv_symm_2axis_angle']\n    df['tbp_lv_symm_2axis/cos_tbp_lv_symm_2axis_angle']=df['tbp_lv_symm_2axis']/df['cos_tbp_lv_symm_2axis_angle']\n    \n    print(\"tbp_lv_x', 'tbp_lv_y', 'tbp_lv_z feature\")\n    #x,y,z也许是长方体的长宽高?用求体积和表面积的特征构造方法试试\n    df['V_tbp_lv']=abs(df['tbp_lv_x']*df['tbp_lv_y']*df['tbp_lv_z'])\n    df['S_tbp_lv']=2*(abs(df['tbp_lv_x']*df['tbp_lv_y'])+abs(df['tbp_lv_x']*df['tbp_lv_z'])+abs(df['tbp_lv_y']*df['tbp_lv_z']))\n    \n    print(\"copyright feature\")\n    cols=['CC-BY','CC-BY-NC','CC-0']\n    for col in cols:\n        df[f'copyright_license_{col}']=(df['copyright_license']==col).astype(np.int8)\n    value_counts={'CC-BY': 188812, 'CC-BY-NC': 183582, 'CC-0': 28665}\n    df['copyright_license']=df['copyright_license'].apply(lambda x:value_counts.get(x,28665))\n    \n    print(\"attribution feature\")\n    value_counts={'Memorial Sloan Kettering Cancer Center': 129068,\n         'Department of Dermatology, Hospital Clínic de Barcelona': 105724,\n         'University Hospital of Basel': 65218,\n         'Frazer Institute, The University of Queensland, Dermatology Research Centre': 51768,\n         'ACEMID MIA': 28665,\n         'ViDIR Group, Department of Dermatology, Medical University of Vienna': 12640,\n         'Department of Dermatology, University of Athens, Andreas Syggros Hospital of Skin and Venereal Diseases, Alexander Stratigos, Konstantinos Liopyris': 7976}\n    for key,value in value_counts.items():\n        df[f\"attribution_{key}\"]=(df['attribution']==key).astype(np.int8)\n    df['attribution']=df['attribution'].apply(lambda x:value_counts.get(x,7976))\n    \n    print(\"tbp_lv_location feature\")\n    value_counts={'Torso Back Top Third': 71112,\n     'Torso Front Top Half': 63350,\n     'Torso Back Middle Third': 46185,\n     'Left Leg - Lower': 27428,\n     'Right Leg - Lower': 25208,\n     'Torso Front Bottom Half': 24360,\n     'Left Leg - Upper': 23673,\n     'Right Leg - Upper': 23034,\n     'Right Arm - Upper': 22972,\n     'Left Arm - Upper': 22816,\n     'Head & Neck': 12046,\n     'Left Arm - Lower': 11939,\n     'Right Arm - Lower': 10636,\n     'Unknown': 5756,\n     'Torso Back Bottom Third': 4596,\n     'Left Leg': 1974,\n     'Right Leg': 1711,\n     'Left Arm': 1593,\n     'Right Arm': 601,\n     'Torso Front': 60,\n     'Torso Back': 9}\n    for key,value in value_counts.items():\n        df[f\"tbp_lv_location_{key}\"]=(df['tbp_lv_location']==key).astype(np.int8)\n    #训练集没有出现过的key假设为训练集里最少的value\n    df['tbp_lv_location']=df['tbp_lv_location'].apply(lambda x:value_counts.get(x,9))\n    \n    value_counts={'Torso Back': 121902,\n     'Torso Front': 87770,\n     'Left Leg': 53075,\n     'Right Leg': 49953,\n     'Left Arm': 36348,\n     'Right Arm': 34209,\n     'Head & Neck': 12046,\n     'Unknown': 5756}    \n    for key,value in value_counts.items():\n        df[f\"tbp_lv_location_simple_{key}\"]=(df['tbp_lv_location_simple']==key).astype(np.int8)\n    #训练集没有出现过的key假设为训练集里最少的value\n    df['tbp_lv_location_simple']=df['tbp_lv_location_simple'].apply(lambda x:value_counts.get(x,5756))\n    \n    print(\"group feature\")\n    #tbp_lv_dnn_lesion_confidence\n    float_cols=['clin_size_long_diam_mm', 'tbp_lv_A', 'tbp_lv_Aext', 'tbp_lv_B', 'tbp_lv_Bext', 'tbp_lv_C', 'tbp_lv_Cext', 'tbp_lv_H', 'tbp_lv_Hext', 'tbp_lv_L', 'tbp_lv_Lext', 'tbp_lv_areaMM2', 'tbp_lv_area_perim_ratio', 'tbp_lv_color_std_mean', 'tbp_lv_deltaA', 'tbp_lv_deltaB', 'tbp_lv_deltaL', 'tbp_lv_deltaLB', 'tbp_lv_deltaLBnorm', 'tbp_lv_eccentricity', 'tbp_lv_minorAxisMM', 'tbp_lv_nevi_confidence', 'tbp_lv_norm_border', 'tbp_lv_norm_color', 'tbp_lv_perimeterMM', 'tbp_lv_radial_color_std_max', 'tbp_lv_stdL', 'tbp_lv_stdLExt', 'tbp_lv_symm_2axis', 'tbp_lv_x', 'tbp_lv_y', 'tbp_lv_z', 'tbp_lv_A+tbp_lv_Aext', 'tbp_lv_A-tbp_lv_Aext', 'tbp_lv_A*tbp_lv_Aext', 'tbp_lv_A/tbp_lv_Aext', 'tbp_lv_B+tbp_lv_Bext', 'tbp_lv_B-tbp_lv_Bext', 'tbp_lv_B*tbp_lv_Bext', 'tbp_lv_B/tbp_lv_Bext', 'tbp_lv_C+tbp_lv_Cext', 'tbp_lv_C-tbp_lv_Cext', 'tbp_lv_C*tbp_lv_Cext', 'tbp_lv_C/tbp_lv_Cext', 'tbp_lv_H+tbp_lv_Hext', 'tbp_lv_H-tbp_lv_Hext', 'tbp_lv_H*tbp_lv_Hext', 'tbp_lv_H/tbp_lv_Hext', 'tbp_lv_L+tbp_lv_Lext', 'tbp_lv_L-tbp_lv_Lext', 'tbp_lv_L*tbp_lv_Lext', 'tbp_lv_L/tbp_lv_Lext', 'sin_tbp_lv_symm_2axis_angle', 'cos_tbp_lv_symm_2axis_angle', 'tbp_lv_symm_2axis*sin_tbp_lv_symm_2axis_angle', 'tbp_lv_symm_2axis*cos_tbp_lv_symm_2axis_angle', 'tbp_lv_symm_2axis/sin_tbp_lv_symm_2axis_angle', 'tbp_lv_symm_2axis/cos_tbp_lv_symm_2axis_angle', 'V_tbp_lv', 'S_tbp_lv']\n    for col in float_cols:\n        df[f\"mean_patient_id_{col}\"]=df.groupby('patient_id')[col].transform('mean')\n        df[f\"min_patient_id_{col}\"]=df.groupby('patient_id')[col].transform('min')\n        df[f\"max_patient_id_{col}\"]=df.groupby('patient_id')[col].transform('max')\n        df[f\"std_patient_id_{col}\"]=df.groupby('patient_id')[col].transform('std')\n        df[f\"median_patient_id_{col}\"]=df.groupby('patient_id')[col].transform('median')\n        df[f\"skew_patient_id_{col}\"]=df.groupby('patient_id')[col].transform('skew')\n\n        df[f\"{col}-mean_patient_id_{col}/std_patient_id_{col}\"]=(df[col]-df[f\"mean_patient_id_{col}\"])/(df[f'std_patient_id_{col}']+1e-15)\n        df[f\"{col}/mean_patient_id_{col}\"]=df[col]/(df[f\"mean_patient_id_{col}\"]+1e-15)\n        df[f'ptp_patient_id_{col}']=df[f\"max_patient_id_{col}\"]-df[f\"min_patient_id_{col}\"]\n        \n    #patient_id的count特征\n    tmp=df.groupby('patient_id')['tbp_lv_B'].count().reset_index().rename(columns={\"tbp_lv_B\":\"tbp_lv_B_count\"})\n    df=df.merge(tmp,on='patient_id',how='left')\n    \n    print(\"-\"*30)\n    return df\ntrain=FE(train)\ntest=FE(test)\n\nprint(\"group age_approx feature\")\nfloat_cols=['clin_size_long_diam_mm', 'tbp_lv_A', 'tbp_lv_Aext', 'tbp_lv_B', 'tbp_lv_Bext', 'tbp_lv_C', 'tbp_lv_Cext', 'tbp_lv_H', 'tbp_lv_Hext', 'tbp_lv_L', 'tbp_lv_Lext', 'tbp_lv_areaMM2', 'tbp_lv_area_perim_ratio', 'tbp_lv_color_std_mean', 'tbp_lv_deltaA', 'tbp_lv_deltaB', 'tbp_lv_deltaL', 'tbp_lv_deltaLB', 'tbp_lv_deltaLBnorm', 'tbp_lv_eccentricity', 'tbp_lv_minorAxisMM', 'tbp_lv_nevi_confidence', 'tbp_lv_norm_border', 'tbp_lv_norm_color', 'tbp_lv_perimeterMM', 'tbp_lv_radial_color_std_max', 'tbp_lv_stdL', 'tbp_lv_stdLExt', 'tbp_lv_symm_2axis', 'tbp_lv_x', 'tbp_lv_y', 'tbp_lv_z', 'tbp_lv_A+tbp_lv_Aext', 'tbp_lv_A-tbp_lv_Aext', 'tbp_lv_A*tbp_lv_Aext', 'tbp_lv_A/tbp_lv_Aext', 'tbp_lv_B+tbp_lv_Bext', 'tbp_lv_B-tbp_lv_Bext', 'tbp_lv_B*tbp_lv_Bext', 'tbp_lv_B/tbp_lv_Bext', 'tbp_lv_C+tbp_lv_Cext', 'tbp_lv_C-tbp_lv_Cext', 'tbp_lv_C*tbp_lv_Cext', 'tbp_lv_C/tbp_lv_Cext', 'tbp_lv_H+tbp_lv_Hext', 'tbp_lv_H-tbp_lv_Hext', 'tbp_lv_H*tbp_lv_Hext', 'tbp_lv_H/tbp_lv_Hext', 'tbp_lv_L+tbp_lv_Lext', 'tbp_lv_L-tbp_lv_Lext', 'tbp_lv_L*tbp_lv_Lext', 'tbp_lv_L/tbp_lv_Lext', 'sin_tbp_lv_symm_2axis_angle', 'cos_tbp_lv_symm_2axis_angle', 'tbp_lv_symm_2axis*sin_tbp_lv_symm_2axis_angle', 'tbp_lv_symm_2axis*cos_tbp_lv_symm_2axis_angle', 'tbp_lv_symm_2axis/sin_tbp_lv_symm_2axis_angle', 'tbp_lv_symm_2axis/cos_tbp_lv_symm_2axis_angle', 'V_tbp_lv', 'S_tbp_lv']\nfor col in tqdm(float_cols):\n    \n    tmp=train.groupby(['age_approx'])[col].mean().reset_index().rename(columns={col:f\"mean_age_approx_{col}\"})\n    train=train.merge(tmp,on='age_approx',how='left')\n    test=test.merge(tmp,on='age_approx',how='left')\n    \n    tmp=train.groupby(['age_approx'])[col].min().reset_index().rename(columns={col:f\"min_age_approx_{col}\"})\n    train=train.merge(tmp,on='age_approx',how='left')\n    test=test.merge(tmp,on='age_approx',how='left')\n    \n    tmp=train.groupby(['age_approx'])[col].max().reset_index().rename(columns={col:f\"max_age_approx_{col}\"})\n    train=train.merge(tmp,on='age_approx',how='left')\n    test=test.merge(tmp,on='age_approx',how='left')\n    \n    tmp=train.groupby(['age_approx'])[col].std().reset_index().rename(columns={col:f\"std_age_approx_{col}\"})\n    train=train.merge(tmp,on='age_approx',how='left')\n    test=test.merge(tmp,on='age_approx',how='left')\n    \n    tmp=train.groupby(['age_approx'])[col].skew().reset_index().rename(columns={col:f\"skew_age_approx_{col}\"})\n    train=train.merge(tmp,on='age_approx',how='left')\n    test=test.merge(tmp,on='age_approx',how='left')\n    \n    tmp=train.groupby(['age_approx'])[col].median().reset_index().rename(columns={col:f\"median_age_approx_{col}\"})\n    train=train.merge(tmp,on='age_approx',how='left')\n    test=test.merge(tmp,on='age_approx',how='left') \n    \n#     train[f\"{col}-mean_age_approx_{col}/std_age_approx_{col}\"]=(train[col]-train[f\"mean_age_approx_{col}\"])/(train[f\"std_age_approx_{col}\"]+1e-15)\n#     test[f\"{col}-mean_age_approx_{col}/std_age_approx_{col}\"]=(test[col]-test[f\"mean_age_approx_{col}\"])/(test[f\"std_age_approx_{col}\"]+1e-15)\n#     train[f\"{col}/mean_age_approx_{col}\"]=train[col]/(train[f\"mean_age_approx_{col}\"]+1e-15)\n#     test[f\"{col}/mean_age_approx_{col}\"]=test[col]/(train[f\"mean_age_approx_{col}\"]+1e-15)\n#     train[f'ptp_age_approx_{col}']=train[f\"max_age_approx_{col}\"]-train[f\"min_age_approx_{col}\"]\n#     test[f'ptp_age_approx_{col}']=test[f\"max_age_approx_{col}\"]-test[f\"min_age_approx_{col}\"]\n        \nprint(\"shift feature\")\ntrain=train.sort_values(['patient_id','age_approx'])\ntest=test.sort_values(['patient_id','age_approx'])\nfor gap in [1]:\n    for col in tqdm(float_cols):\n        train[f\"{col}_diff_{gap}\"]=train.groupby(['patient_id'])[col].diff(gap)\n        train[f\"{col}_diff_{gap}\"]=train[f\"{col}_diff_{gap}\"].fillna(0)\n        test[f\"{col}_diff_{gap}\"]=test.groupby(['patient_id'])[col].diff(gap)\n        test[f\"{col}_diff_{gap}\"]=test[f\"{col}_diff_{gap}\"].fillna(0)\n        \n        train[f\"{col}_groupanatom_site_general_diff_{gap}\"]=train.groupby(['patient_id','anatom_site_general'])[col].diff(gap)\n        train[f\"{col}_groupanatom_site_general_diff_{gap}\"]=train[f\"{col}_groupanatom_site_general_diff_{gap}\"].fillna(0)\n        test[f\"{col}_groupanatom_site_general_diff_{gap}\"]=test.groupby(['patient_id','anatom_site_general'])[col].diff(gap)\n        test[f\"{col}_groupanatom_site_general_diff_{gap}\"]=test[f\"{col}_groupanatom_site_general_diff_{gap}\"].fillna(0)\n        \n\ntrain.replace([np.inf, -np.inf], np.nan, inplace=True)    \ntest.replace([np.inf, -np.inf], np.nan, inplace=True)\n\nfor col in test.drop(['isic_id','patient_id','combined_anatomical_site'],axis=1).columns:\n    skew=train[col].skew()\n    if abs(skew)>0.5:\n        min_value=train[col].min()\n        train[col]=train[col]-min_value\n        test[col]=test[col]-min_value\n        \n        #print(f\"col:{col},skew:{skew}\")\n        train[col]=np.log1p(train[col])\n        test[col]=np.log1p(test[col])\n\ntrain.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Although this dataset is not large and theoretically does not exceed memory capacity, this function was still used.","metadata":{}},{"cell_type":"code","source":"#遍历表格df的所有列修改数据类型减少内存使用\ndef reduce_mem_usage(df, float16_as32=True):\n    #memory_usage()是df每列的内存使用量,sum是对它们求和, B->KB->MB\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:#遍历每列的列名\n        col_type = df[col].dtype#列名的type\n        if col_type != object and str(col_type)!='category':#不是object也就是说这里处理的是数值类型的变量\n            c_min,c_max = df[col].min(),df[col].max() #求出这列的最大值和最小值\n            if str(col_type)[:3] == 'int':#如果是int类型的变量,不管是int8,int16,int32还是int64\n                #如果这列的取值范围是在int8的取值范围内,那就对类型进行转换 (-128 到 127)\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                #如果这列的取值范围是在int16的取值范围内,那就对类型进行转换(-32,768 到 32,767)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                #如果这列的取值范围是在int32的取值范围内,那就对类型进行转换(-2,147,483,648到2,147,483,647)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                #如果这列的取值范围是在int64的取值范围内,那就对类型进行转换(-9,223,372,036,854,775,808到9,223,372,036,854,775,807)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:#如果是浮点数类型.\n                #如果数值在float16的取值范围内,如果觉得需要更高精度可以考虑float32\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    if float16_as32:#如果数据需要更高的精度可以选择float32\n                        df[col] = df[col].astype(np.float32)\n                    else:\n                        df[col] = df[col].astype(np.float16)  \n                #如果数值在float32的取值范围内，对它进行类型转换\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                #如果数值在float64的取值范围内，对它进行类型转换\n                else:\n                    df[col] = df[col].astype(np.float64)\n    #计算一下结束后的内存\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    #相比一开始的内存减少了百分之多少\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df\ntrain=reduce_mem_usage(train, float16_as32=True)\ntest=reduce_mem_usage(test, float16_as32=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Metric\n\n- <a href=\"https://www.kaggle.com/code/yunsuxiaozi/isic-2024-metric-pauc\">isic-2024-metric-pauc</a>","metadata":{}},{"cell_type":"code","source":"# Using the official evaluation metric: pAUC\ndef pauc_above_tpr(y_true, y_pred):\n    min_tpr = 0.8  # Minimum True Positive Rate (TPR)\n    v_gt = abs(np.asarray(y_true) - 1)  # Convert ground truth to binary (0 and 1)\n    v_pred = np.array([1.0 - x for x in y_pred])  # Convert predictions to probabilities\n    max_fpr = abs(1 - min_tpr)  # Calculate maximum False Positive Rate (FPR)\n    \n    # Calculate the scaled partial AUC\n    partial_auc_scaled = roc_auc_score(v_gt, v_pred, max_fpr=max_fpr)\n    \n    # Calculate the final partial AUC\n    partial_auc = 0.5 * max_fpr**2 + (max_fpr - 0.5 * max_fpr**2) / (1.0 - 0.5) * (partial_auc_scaled - 0.5)\n    \n    return 'pauc', partial_auc, True","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Training\n\n- We use 'GroupKFold' here to prevent data leakage between patients.Due to the involvement of different patients in the training and testing sets, the label distribution varies among different patients, therefore 'StratifiedGroupKFold' is not used.\n\n- Due to the small proportion of positive samples, 'early_stop' is not used here.\n\n- If the data is not undersampled, the sample will be imbalanced; If undersampling the data leads to a small total sample size and poor performance of the trained model, we will sample 100000 negative samples and weight the positive samples.","metadata":{}},{"cell_type":"code","source":"# Select columns for modeling that are not of object type\nchoose_cols = [col for col in test.columns if train[col].dtype != object]\n\n# Dropping columns with high correlation\n# drop_cols = []\n# metric = train[choose_cols].corr().values\n# for i in range(len(metric)):\n#     for j in range(i + 1, len(metric)):\n#         if abs(metric[i][j]) > 0.99:\n#             drop_cols += [choose_cols[j]]\n#     print(f\"i:{i}\")\n# print(f\"drop_cols={drop_cols}\")\n\n# Retain only one column among those with high correlation with others\ndrop_cols = ['mean_sex_tbp_lv_dnn_lesion_confidence', 'min_sex_tbp_lv_dnn_lesion_confidence', \n             'median_sex_tbp_lv_dnn_lesion_confidence', 'std_sex_tbp_lv_dnn_lesion_confidence', \n             'skew_sex_tbp_lv_dnn_lesion_confidence', 'mean_tbp_tile_type_tbp_lv_dnn_lesion_confidence', \n             'min_tbp_tile_type_tbp_lv_dnn_lesion_confidence', 'median_tbp_tile_type_tbp_lv_dnn_lesion_confidence', \n             'std_tbp_tile_type_tbp_lv_dnn_lesion_confidence', 'skew_tbp_tile_type_tbp_lv_dnn_lesion_confidence', \n             'tbp_lv_L+tbp_lv_Lext', 'lesion_shape_index', 'tbp_lv_A-tbp_lv_Aext', 'tbp_lv_B-tbp_lv_Bext', \n             'tbp_lv_L-tbp_lv_Lext', 'border_complexity', 'shape_complexity_index', \n             'min_copyright_license_tbp_lv_dnn_lesion_confidence', 'skew_copyright_license_tbp_lv_dnn_lesion_confidence', \n             'copyright_license_CC-0', 'attribution_ACEMID MIA', 'std_dev_contrast', 'shape_complexity_index', \n             'tbp_lv_H+tbp_lv_Hext', 'tbp_lv_H*tbp_lv_Hext', 'min_sex_tbp_lv_dnn_lesion_confidence', \n             'median_sex_tbp_lv_dnn_lesion_confidence', 'std_sex_tbp_lv_dnn_lesion_confidence', \n             'skew_sex_tbp_lv_dnn_lesion_confidence', 'sex_male', 'sex_female', 'median_sex_tbp_lv_dnn_lesion_confidence', \n             'std_sex_tbp_lv_dnn_lesion_confidence', 'skew_sex_tbp_lv_dnn_lesion_confidence', 'sex_male', 'sex_female', \n             'std_sex_tbp_lv_dnn_lesion_confidence', 'skew_sex_tbp_lv_dnn_lesion_confidence', 'sex_male', 'sex_female', \n             'skew_sex_tbp_lv_dnn_lesion_confidence', 'sex_male', 'sex_female', 'sex_male', 'sex_female', \n             'mean_tbp_lv_location_simple_tbp_lv_dnn_lesion_confidence', 'anatom_site_general_upper extremity', \n             'median_tbp_lv_location_simple_tbp_lv_dnn_lesion_confidence', 'std_tbp_lv_location_simple_tbp_lv_dnn_lesion_confidence', \n             'skew_tbp_lv_location_simple_tbp_lv_dnn_lesion_confidence', 'min_tbp_tile_type_tbp_lv_dnn_lesion_confidence', \n             'median_tbp_tile_type_tbp_lv_dnn_lesion_confidence', 'std_tbp_tile_type_tbp_lv_dnn_lesion_confidence', \n             'skew_tbp_tile_type_tbp_lv_dnn_lesion_confidence', 'median_tbp_tile_type_tbp_lv_dnn_lesion_confidence', \n             'std_tbp_tile_type_tbp_lv_dnn_lesion_confidence', 'skew_tbp_tile_type_tbp_lv_dnn_lesion_confidence', \n             'std_tbp_tile_type_tbp_lv_dnn_lesion_confidence', 'skew_tbp_tile_type_tbp_lv_dnn_lesion_confidence', \n             'skew_tbp_tile_type_tbp_lv_dnn_lesion_confidence', 'tbp_lv_location_Unknown', \n             'tbp_lv_location_simple_Unknown', 'attribution_ViDIR Group, Department of Dermatology, Medical University of Vienna', \n             'std_copyright_license_tbp_lv_dnn_lesion_confidence', 'skew_copyright_license_tbp_lv_dnn_lesion_confidence', \n             'copyright_license_CC-0', 'attribution_ACEMID MIA', 'copyright_license_CC-BY', 'copyright_license_CC-0', \n             'attribution_ACEMID MIA', 'tbp_lv_location_simple_Torso Back', 'tbp_lv_location_simple_Torso Front', \n             'tbp_lv_location_Head & Neck', 'tbp_lv_location_simple_Head & Neck', 'tbp_lv_B*tbp_lv_Bext', 'tbp_lv_C*tbp_lv_Cext', \n             'tbp_lv_H*tbp_lv_Hext', 'sin_tbp_lv_symm_2axis_angle-mean_patient_id_sin_tbp_lv_symm_2axis_angle/std_patient_id_sin_tbp_lv_symm_2axis_angle', \n             'cos_tbp_lv_symm_2axis_angle-mean_patient_id_cos_tbp_lv_symm_2axis_angle/std_patient_id_cos_tbp_lv_symm_2axis_angle', \n             'attribution_ACEMID MIA', 'tbp_lv_location_simple_Head & Neck', 'tbp_lv_location_simple_Unknown', \n             'clin_size_long_diam_mm/mean_patient_id_clin_size_long_diam_mm', 'median_patient_id_tbp_lv_A', \n             'median_patient_id_tbp_lv_Aext', 'median_patient_id_tbp_lv_B', 'median_patient_id_tbp_lv_Bext', \n             'median_patient_id_tbp_lv_C', 'median_patient_id_tbp_lv_Cext', 'median_patient_id_tbp_lv_Hext', \n             'median_patient_id_tbp_lv_L', 'mean_patient_id_tbp_lv_L+tbp_lv_Lext', 'median_patient_id_tbp_lv_L+tbp_lv_Lext', \n             'max_patient_id_tbp_lv_L+tbp_lv_Lext', 'std_patient_id_tbp_lv_L+tbp_lv_Lext', 'mean_patient_id_tbp_lv_L+tbp_lv_Lext', \n             'median_patient_id_tbp_lv_L+tbp_lv_Lext', 'median_patient_id_tbp_lv_Lext', 'mean_patient_id_tbp_lv_L+tbp_lv_Lext', \n             'median_patient_id_tbp_lv_L+tbp_lv_Lext', 'std_patient_id_tbp_lv_L+tbp_lv_Lext', 'mean_patient_id_tbp_lv_L+tbp_lv_Lext', \n             'median_patient_id_tbp_lv_L+tbp_lv_Lext', 'tbp_lv_areaMM2/mean_patient_id_tbp_lv_areaMM2', \n             'median_patient_id_tbp_lv_area_perim_ratio', 'mean_patient_id_tbp_lv_norm_color', \n             'ptp_patient_id_tbp_lv_color_std_mean', 'mean_patient_id_tbp_lv_A-tbp_lv_Aext', 'min_patient_id_tbp_lv_A-tbp_lv_Aext', \n             'max_patient_id_tbp_lv_A-tbp_lv_Aext', 'std_patient_id_tbp_lv_A-tbp_lv_Aext', 'median_patient_id_tbp_lv_A-tbp_lv_Aext', \n             'skew_patient_id_tbp_lv_A-tbp_lv_Aext', 'tbp_lv_A-tbp_lv_Aext-mean_patient_id_tbp_lv_A-tbp_lv_Aext/std_patient_id_tbp_lv_A-tbp_lv_Aext', \n             'tbp_lv_A-tbp_lv_Aext/mean_patient_id_tbp_lv_A-tbp_lv_Aext', 'ptp_patient_id_tbp_lv_A-tbp_lv_Aext', \n             'mean_patient_id_tbp_lv_B-tbp_lv_Bext', 'min_patient_id_tbp_lv_B-tbp_lv_Bext', 'max_patient_id_tbp_lv_B-tbp_lv_Bext', \n             'std_patient_id_tbp_lv_B-tbp_lv_Bext', 'median_patient_id_tbp_lv_B-tbp_lv_Bext', 'skew_patient_id_tbp_lv_B-tbp_lv_Bext', \n             'tbp_lv_B-tbp_lv_Bext-mean_patient_id_tbp_lv_B-tbp_lv_Bext/std_patient_id_tbp_lv_B-tbp_lv_Bext', \n             'tbp_lv_B-tbp_lv_Bext/mean_patient_id_tbp_lv_B-tbp_lv_Bext', 'ptp_patient_id_tbp_lv_B-tbp_lv_Bext', \n             'mean_patient_id_tbp_lv_deltaLB', 'mean_patient_id_tbp_lv_L-tbp_lv_Lext', 'min_patient_id_tbp_lv_L-tbp_lv_Lext', \n             'max_patient_id_tbp_lv_L-tbp_lv_Lext', 'std_patient_id_tbp_lv_deltaLB', 'std_patient_id_tbp_lv_L-tbp_lv_Lext', \n             'median_patient_id_tbp_lv_L-tbp_lv_Lext', 'skew_patient_id_tbp_lv_L-tbp_lv_Lext', 'tbp_lv_L-tbp_lv_Lext-mean_patient_id_tbp_lv_L-tbp_lv_Lext/std_patient_id_tbp_lv_L-tbp_lv_Lext', \n             'tbp_lv_L-tbp_lv_Lext/mean_patient_id_tbp_lv_L-tbp_lv_Lext', 'ptp_patient_id_tbp_lv_L-tbp_lv_Lext', \n             'mean_patient_id_tbp_lv_L-tbp_lv_Lext', 'std_patient_id_tbp_lv_L-tbp_lv_Lext', 'tbp_lv_eccentricity/mean_patient_id_tbp_lv_eccentricity', \n             'ptp_patient_id_tbp_lv_minorAxisMM', 'median_patient_id_tbp_lv_norm_border', 'mean_patient_id_tbp_lv_symm_2axis', \n             'tbp_lv_norm_border/mean_patient_id_tbp_lv_norm_border', 'mean_patient_id_tbp_lv_radial_color_std_max', \n             'ptp_patient_id_tbp_lv_perimeterMM', 'ptp_patient_id_tbp_lv_radial_color_std_max', 'ptp_patient_id_tbp_lv_stdL', \n             'ptp_patient_id_tbp_lv_stdLExt', 'median_patient_id_tbp_lv_symm_2axis', 'tbp_lv_symm_2axis/mean_patient_id_tbp_lv_symm_2axis', \n             'median_patient_id_tbp_lv_A+tbp_lv_Aext', 'tbp_lv_A*tbp_lv_Aext-mean_patient_id_tbp_lv_A*tbp_lv_Aext/std_patient_id_tbp_lv_A*tbp_lv_Aext', \n             'tbp_lv_A*tbp_lv_Aext/mean_patient_id_tbp_lv_A*tbp_lv_Aext', 'median_patient_id_tbp_lv_A*tbp_lv_Aext', \n             'median_patient_id_tbp_lv_B+tbp_lv_Bext', 'max_patient_id_tbp_lv_B*tbp_lv_Bext', 'tbp_lv_B*tbp_lv_Bext-mean_patient_id_tbp_lv_B*tbp_lv_Bext/std_patient_id_tbp_lv_B*tbp_lv_Bext', \n             'tbp_lv_B*tbp_lv_Bext/mean_patient_id_tbp_lv_B*tbp_lv_Bext', 'median_patient_id_tbp_lv_B*tbp_lv_Bext', \n             'median_patient_id_tbp_lv_C+tbp_lv_Cext', 'max_patient_id_tbp_lv_C*tbp_lv_Cext', \n             'tbp_lv_C*tbp_lv_Cext-mean_patient_id_tbp_lv_C*tbp_lv_Cext/std_patient_id_tbp_lv_C*tbp_lv_Cext', \n             'tbp_lv_C*tbp_lv_Cext/mean_patient_id_tbp_lv_C*tbp_lv_Cext', 'median_patient_id_tbp_lv_C-tbp_lv_Cext', \n             'median_patient_id_tbp_lv_C*tbp_lv_Cext', 'median_patient_id_tbp_lv_C/tbp_lv_Cext', 'median_patient_id_tbp_lv_H+tbp_lv_Hext', \n             'mean_patient_id_tbp_lv_H*tbp_lv_Hext', 'median_patient_id_tbp_lv_H*tbp_lv_Hext', 'min_patient_id_tbp_lv_H*tbp_lv_Hext', \n             'mean_patient_id_tbp_lv_H*tbp_lv_Hext', 'median_patient_id_tbp_lv_H*tbp_lv_Hext', 'tbp_lv_H*tbp_lv_Hext-mean_patient_id_tbp_lv_H*tbp_lv_Hext/std_patient_id_tbp_lv_H*tbp_lv_Hext', \n             'tbp_lv_H*tbp_lv_Hext/mean_patient_id_tbp_lv_H*tbp_lv_Hext', 'median_patient_id_tbp_lv_H*tbp_lv_Hext', \n             'median_patient_id_tbp_lv_L+tbp_lv_Lext', 'max_patient_id_tbp_lv_L*tbp_lv_Lext', 'tbp_lv_L*tbp_lv_Lext-mean_patient_id_tbp_lv_L*tbp_lv_Lext/std_patient_id_tbp_lv_L*tbp_lv_Lext', \n             'tbp_lv_L*tbp_lv_Lext/mean_patient_id_tbp_lv_L*tbp_lv_Lext', 'median_patient_id_tbp_lv_L*tbp_lv_Lext', \n             'std_patient_id_tbp_lv_symm_2axis/sin_tbp_lv_symm_2axis_angle', 'ptp_patient_id_tbp_lv_symm_2axis/sin_tbp_lv_symm_2axis_angle', \n             'ptp_patient_id_V_tbp_lv', 'ptp_patient_id_S_tbp_lv', 'median_age_approx_tbp_lv_Bext', 'median_age_approx_tbp_lv_C', \n             'median_age_approx_tbp_lv_Hext', 'skew_age_approx_tbp_lv_L+tbp_lv_Lext', 'median_age_approx_tbp_lv_L+tbp_lv_Lext', \n             'median_age_approx_tbp_lv_minorAxisMM', 'skew_age_approx_tbp_lv_area_perim_ratio', 'median_age_approx_tbp_lv_area_perim_ratio', \n             'mean_age_approx_tbp_lv_nevi_confidence', 'skew_age_approx_tbp_lv_nevi_confidence', 'mean_age_approx_tbp_lv_norm_border', \n             'median_age_approx_tbp_lv_norm_border', 'mean_age_approx_tbp_lv_symm_2axis', 'median_age_approx_tbp_lv_symm_2axis', \n             'std_age_approx_tbp_lv_symm_2axis*sin_tbp_lv_symm_2axis_angle', 'mean_age_approx_tbp_lv_norm_border', \n             'skew_age_approx_tbp_lv_norm_border', 'skew_age_approx_tbp_lv_symm_2axis', 'mean_age_approx_tbp_lv_nevi_confidence', \n             'skew_age_approx_tbp_lv_nevi_confidence', 'mean_age_approx_tbp_lv_norm_border', 'median_age_approx_tbp_lv_norm_border', \n             'median_age_approx_tbp_lv_symm_2axis', 'mean_age_approx_tbp_lv_norm_color', 'median_age_approx_tbp_lv_radial_color_std_max', \n             'median_age_approx_tbp_lv_norm_color', 'median_age_approx_tbp_lv_radial_color_std_max', 'median_age_approx_tbp_lv_deltaA', \n             'mean_age_approx_tbp_lv_A-tbp_lv_Aext', 'median_age_approx_tbp_lv_A-tbp_lv_Aext', 'min_age_approx_tbp_lv_A-tbp_lv_Aext', \n             'max_age_approx_tbp_lv_A-tbp_lv_Aext', 'std_age_approx_tbp_lv_A-tbp_lv_Aext', 'skew_age_approx_tbp_lv_A-tbp_lv_Aext', \n             'mean_age_approx_tbp_lv_A-tbp_lv_Aext', 'median_age_approx_tbp_lv_A-tbp_lv_Aext', 'mean_age_approx_tbp_lv_B-tbp_lv_Bext', \n             'median_age_approx_tbp_lv_B/tbp_lv_Bext', 'min_age_approx_tbp_lv_B-tbp_lv_Bext', 'max_age_approx_tbp_lv_B-tbp_lv_Bext', \n             'std_age_approx_tbp_lv_B-tbp_lv_Bext', 'skew_age_approx_tbp_lv_B-tbp_lv_Bext', 'median_age_approx_tbp_lv_B-tbp_lv_Bext', \n             'median_age_approx_tbp_lv_deltaL', 'mean_age_approx_tbp_lv_L-tbp_lv_Lext', 'median_age_approx_tbp_lv_L-tbp_lv_Lext', \n             'min_age_approx_tbp_lv_L-tbp_lv_Lext', 'max_age_approx_tbp_lv_L-tbp_lv_Lext', 'std_age_approx_tbp_lv_deltaLB', \n             'std_age_approx_tbp_lv_L-tbp_lv_Lext', 'skew_age_approx_tbp_lv_deltaLB', 'skew_age_approx_tbp_lv_L-tbp_lv_Lext', \n             'mean_age_approx_tbp_lv_L-tbp_lv_Lext', 'median_age_approx_tbp_lv_L-tbp_lv_Lext', 'median_age_approx_tbp_lv_deltaLB', \n             'std_age_approx_tbp_lv_L-tbp_lv_Lext', 'skew_age_approx_tbp_lv_deltaLBnorm', 'skew_age_approx_tbp_lv_L-tbp_lv_Lext', \n             'median_age_approx_tbp_lv_deltaLBnorm', 'mean_age_approx_tbp_lv_stdL', 'median_age_approx_tbp_lv_stdL', \n             'mean_age_approx_tbp_lv_stdL', 'median_age_approx_tbp_lv_stdL', 'skew_age_approx_tbp_lv_eccentricity', \n             'median_age_approx_tbp_lv_eccentricity', 'median_age_approx_tbp_lv_eccentricity', 'skew_age_approx_tbp_lv_nevi_confidence', \n             'mean_age_approx_tbp_lv_norm_border', 'skew_age_approx_tbp_lv_norm_border', 'median_age_approx_tbp_lv_norm_border', \n             'mean_age_approx_tbp_lv_symm_2axis', 'skew_age_approx_tbp_lv_symm_2axis', 'median_age_approx_tbp_lv_symm_2axis', \n             'std_age_approx_tbp_lv_symm_2axis*sin_tbp_lv_symm_2axis_angle', 'std_age_approx_tbp_lv_symm_2axis*cos_tbp_lv_symm_2axis_angle', \n             'median_age_approx_sin_tbp_lv_symm_2axis_angle', 'median_age_approx_cos_tbp_lv_symm_2axis_angle', \n             'median_age_approx_tbp_lv_symm_2axis*sin_tbp_lv_symm_2axis_angle', 'median_age_approx_tbp_lv_symm_2axis*cos_tbp_lv_symm_2axis_angle', \n             'mean_age_approx_tbp_lv_symm_2axis/cos_tbp_lv_symm_2axis_angle', 'mean_age_approx_tbp_lv_norm_border', \n             'median_age_approx_tbp_lv_norm_border', 'mean_age_approx_tbp_lv_symm_2axis', 'median_age_approx_tbp_lv_symm_2axis', \n             'skew_age_approx_tbp_lv_norm_border', 'median_age_approx_tbp_lv_norm_border', 'mean_age_approx_tbp_lv_symm_2axis', \n             'skew_age_approx_tbp_lv_symm_2axis', 'median_age_approx_tbp_lv_symm_2axis', 'std_age_approx_tbp_lv_symm_2axis*sin_tbp_lv_symm_2axis_angle', \n             'std_age_approx_tbp_lv_symm_2axis*cos_tbp_lv_symm_2axis_angle', 'mean_age_approx_tbp_lv_symm_2axis', \n             'skew_age_approx_tbp_lv_symm_2axis', 'std_age_approx_tbp_lv_symm_2axis*sin_tbp_lv_symm_2axis_angle', \n             'mean_age_approx_tbp_lv_symm_2axis', 'median_age_approx_tbp_lv_symm_2axis', 'std_age_approx_tbp_lv_symm_2axis*sin_tbp_lv_symm_2axis_angle', \n             'median_age_approx_tbp_lv_norm_color', 'median_age_approx_tbp_lv_radial_color_std_max', 'median_age_approx_tbp_lv_radial_color_std_max', \n             'median_age_approx_tbp_lv_stdL', 'skew_age_approx_tbp_lv_symm_2axis', 'median_age_approx_tbp_lv_symm_2axis', \n             'std_age_approx_tbp_lv_symm_2axis*sin_tbp_lv_symm_2axis_angle', 'std_age_approx_tbp_lv_symm_2axis*cos_tbp_lv_symm_2axis_angle', \n             'std_age_approx_tbp_lv_symm_2axis*sin_tbp_lv_symm_2axis_angle', 'std_age_approx_tbp_lv_symm_2axis*cos_tbp_lv_symm_2axis_angle', \n             'std_age_approx_tbp_lv_symm_2axis*sin_tbp_lv_symm_2axis_angle', 'std_age_approx_tbp_lv_symm_2axis*cos_tbp_lv_symm_2axis_angle', \n             'mean_age_approx_tbp_lv_A*tbp_lv_Aext', 'max_age_approx_tbp_lv_A*tbp_lv_Aext', 'median_age_approx_tbp_lv_A*tbp_lv_Aext', \n             'median_age_approx_tbp_lv_A-tbp_lv_Aext', 'median_age_approx_tbp_lv_A/tbp_lv_Aext', 'mean_age_approx_tbp_lv_B*tbp_lv_Bext', \n             'median_age_approx_tbp_lv_B/tbp_lv_Bext', 'median_age_approx_tbp_lv_C+tbp_lv_Cext', 'mean_age_approx_tbp_lv_C*tbp_lv_Cext', \n             'skew_age_approx_tbp_lv_C*tbp_lv_Cext', 'median_age_approx_tbp_lv_H+tbp_lv_Hext', 'median_age_approx_tbp_lv_H*tbp_lv_Hext', \n             'median_age_approx_tbp_lv_H*tbp_lv_Hext', 'median_age_approx_tbp_lv_H-tbp_lv_Hext', 'skew_age_approx_tbp_lv_H/tbp_lv_Hext', \n             'median_age_approx_tbp_lv_H/tbp_lv_Hext', 'max_age_approx_tbp_lv_L*tbp_lv_Lext', 'median_age_approx_tbp_lv_L-tbp_lv_Lext', \n             'std_age_approx_cos_tbp_lv_symm_2axis_angle', 'median_age_approx_cos_tbp_lv_symm_2axis_angle', \n             'median_age_approx_tbp_lv_symm_2axis*sin_tbp_lv_symm_2axis_angle', 'median_age_approx_tbp_lv_symm_2axis*cos_tbp_lv_symm_2axis_angle', \n             'mean_age_approx_tbp_lv_symm_2axis/cos_tbp_lv_symm_2axis_angle', 'median_age_approx_tbp_lv_symm_2axis*sin_tbp_lv_symm_2axis_angle', \n             'median_age_approx_tbp_lv_symm_2axis*cos_tbp_lv_symm_2axis_angle', 'mean_age_approx_tbp_lv_symm_2axis/cos_tbp_lv_symm_2axis_angle', \n             'std_age_approx_tbp_lv_symm_2axis*cos_tbp_lv_symm_2axis_angle', 'median_age_approx_tbp_lv_symm_2axis*cos_tbp_lv_symm_2axis_angle', \n             'mean_age_approx_tbp_lv_symm_2axis/cos_tbp_lv_symm_2axis_angle', 'mean_age_approx_tbp_lv_symm_2axis/cos_tbp_lv_symm_2axis_angle', \n             'max_age_approx_tbp_lv_symm_2axis/cos_tbp_lv_symm_2axis_angle', 'median_age_approx_V_tbp_lv', 'mean_age_approx_S_tbp_lv', \n             'mean_age_approx_S_tbp_lv', 'tbp_lv_A-tbp_lv_Aext_diff_1', 'tbp_lv_B-tbp_lv_Bext_diff_1', 'tbp_lv_deltaLB_diff_1', \n             'tbp_lv_L-tbp_lv_Lext_diff_1', 'tbp_lv_L-tbp_lv_Lext_diff_1', 'tbp_lv_B*tbp_lv_Bext_diff_1', 'tbp_lv_C*tbp_lv_Cext_diff_1', \n             'tbp_lv_H*tbp_lv_Hext_diff_1']\n\nchoose_cols = [col for col in choose_cols if col not in drop_cols]\nprint(f\"len(choose_cols): {len(choose_cols)}\")\n\ncols_name = {}\nfor i in range(len(choose_cols)):\n    cols_name[choose_cols[i]] = f\"cols_{i}\"\n\ndef fit_and_predict(train_feats=train, test_feats=test, model=None, num_folds=10, name='lgb'):\n    X = train_feats[choose_cols].copy().rename(columns=cols_name)\n    y = train_feats['target'].copy()\n    patient_id = train_feats['patient_id'].copy()\n    oof_pred = np.zeros((len(X)))\n    test_X = test_feats[choose_cols].copy().rename(columns=cols_name)\n    test_pred_pro = np.zeros((num_folds, len(test_X)))\n     \n    # K-fold cross-validation\n    gkf = GroupKFold(n_splits=num_folds) #,shuffle=True\n    for fold, (train_index, valid_index) in enumerate(gkf.split(X, y, patient_id)):\n        print(f\"name {name}, fold: {fold}\")\n\n        X_train, X_valid = X.iloc[train_index].reset_index(drop=True), X.iloc[valid_index].reset_index(drop=True)\n        y_train, y_valid = y.iloc[train_index].reset_index(drop=True), y.iloc[valid_index].reset_index(drop=True)\n        \n        # Sample 100,000 instances from the label 0 data in the training set\n        zero_index = np.where(y_train == 0)[0]\n        one_index = np.where(y_train == 1)[0]\n        np.random.shuffle(zero_index)\n        total_index = list(zero_index[:10000]) + list(one_index)\n        X_train = X_train.iloc[total_index]\n        y_train = y_train.iloc[total_index]\n        \n        if 'lgb' in name:\n            model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)],\n                      callbacks=[log_evaluation(100)],\n                      eval_metric=pauc_above_tpr\n                     )\n        if 'xgb' in name:\n            model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)],\n                      verbose=100,\n                     )\n        if 'cat' in name:\n            model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)],\n                      verbose=100\n                     )\n        \n        oof_pred[valid_index] = model.predict_proba(X_valid)[:, 1]\n        test_pred_pro[fold] = model.predict_proba(test_X)[:, 1]\n\n    test_pred_pro = test_pred_pro.mean(axis=0)\n    print(f\"name: {name}, pauc: {pauc_above_tpr(y.values.astype(np.int8), oof_pred)}\")\n    \n    return oof_pred, test_pred_pro","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Optuna find best\n\n```sh\n- optuna find best lgb_params\n- optuna find best xgb_params\n- optuna find best cat_params\n```\n\n### Sample of Optimization","metadata":{}},{"cell_type":"code","source":"# LightGBM Model - Best Parameters Found by Optuna (Trial 21)\nlgb_params = {\n    \"boosting_type\": \"gbdt\",\n    \"objective\": \"binary\",\n    \"metric\": \"auc\",\n    \"random_state\": 2024,\n    \"n_estimators\": 1000,\n    \"reg_alpha\": 0.3459645276017008,\n    \"reg_lambda\": 0.18711879709643683,\n    \"colsample_bytree\": 0.62892422100233,\n    \"subsample\": 0.6208054621175016,\n    \"learning_rate\": 0.007309181549082522,\n    \"num_leaves\": 38,\n    \"min_child_samples\": 53,\n    \"scale_pos_weight\": 2.5,\n    \"verbose\": -1,\n    'device':'gpu', 'gpu_use_dp':True,  # Uncomment this line if using a GPU environment. Comment it out for CPU.\n}\n\n# Fit and predict using the LightGBM model with the best parameters\nlgb_oof_pred_pro, lgb_test_pro = fit_and_predict(model=LGBMClassifier(**lgb_params), num_folds=Config.num_folds, name='lgb')\nprint(f\"lgb_test_pro[:10]: {lgb_test_pro[:10]}\")\n\n# CatBoost Model - Best Parameters Found by Optuna (Trial 1)\ncat_params = {\n    'iterations': 999,\n    'od_wait': 589,\n    'task_type': \"GPU\",\n    'leaf_estimation_method': 'Newton',\n    'bootstrap_type': 'Bernoulli',\n    'learning_rate': 0.06565361652314616,\n    'reg_lambda': 92.76585571631034,\n    'subsample': 0.8310010342463381,\n    'random_strength': 27.409704119980354,\n    'depth': 7,\n    'min_data_in_leaf': 24,\n    'leaf_estimation_iterations': 13\n}\n\n# Fit and predict using the CatBoost model with the best parameters\ncat_model = CatBoostClassifier(**cat_params)\ncat_oof_pred_pro, cat_test_pro = fit_and_predict(model=cat_model, num_folds=Config.num_folds, name='cat')\nprint(f\"cat_test_pro[:10]: {cat_test_pro[:10]}\")\n\n# XGBoost Model - Best Parameters Found by Optuna (Trial 35)\nxgb_params = {\n    'random_state': 2024,\n    'n_estimators': 760,\n    'learning_rate': 0.009826644028525231,\n    'max_depth': 10,\n    'reg_alpha': 0.08277318651348423,\n    'reg_lambda': 0.7719612355688399,\n    'subsample': 0.9579828266704034,\n    'colsample_bytree': 0.6228502853913586,\n    'min_child_weight': 3,\n    'scale_pos_weight': 2.5,\n    'tree_method': 'gpu_hist',\n    'objective': 'binary:logistic',\n}\n\n# Fit and predict using the XGBoost model with the best parameters\nxgb_model = XGBClassifier(**xgb_params)\nxgb_oof_pred_pro, xgb_test_pro = fit_and_predict(model=xgb_model, num_folds=Config.num_folds, name='xgb')\nprint(f\"xgb_test_pro[:10]: {xgb_test_pro[:10]}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Best Fusion Weight","metadata":{}},{"cell_type":"code","source":"# Model Parameter Selection\nsteps = 100\nbest_w1 = 1\nbest_w2 = 1\n\n# Initial predictions based on default weights\ninit_pros = (best_w1 * lgb_oof_pred_pro + best_w2 * cat_oof_pred_pro + (steps - best_w1 - best_w2) * xgb_oof_pred_pro) / steps\nbest_pauc = pauc_above_tpr(train[Config.TARGET_NAME].values, init_pros)[1]\nprint(f\"Initial best_pauc: {best_pauc}\")\n\n# Skipping the parameter search, using average as the final prediction result\nbest_w1, best_w2 = 33, 33\n\n# # Uncomment the following code to search for the best weights\n# for w1 in range(1, steps - 2):  # Weight for LightGBM\n#     for w2 in range(1, steps - 2):  # Weight for CatBoost\n#         w3 = steps - w1 - w2  # Weight for XGBoost\n#         # Current prediction results\n#         cur_pros = (w1 * lgb_oof_pred_pro + w2 * cat_oof_pred_pro + w3 * xgb_oof_pred_pro) / steps\n#         cur_pauc = pauc_above_tpr(train[Config.TARGET_NAME].values, cur_pros)[1]\n#         if cur_pauc > best_pauc:\n#             best_w1 = w1\n#             best_w2 = w2\n#             best_pauc = cur_pauc\n#             print(f\"Updated best_w1: {best_w1}, best_w2: {best_w2}, best_pauc: {best_pauc}\")\n\n# Final predictions based on the selected best weights\nbest_pros = (best_w1 * lgb_oof_pred_pro + best_w2 * cat_oof_pred_pro + (steps - best_w1 - best_w2) * xgb_oof_pred_pro) / steps\nbest_pauc = pauc_above_tpr(train[Config.TARGET_NAME].values, best_pros)[1]\nprint(f\"Final best_pauc: {best_pauc}\")\n\n# Apply the best weights to the test set predictions\ntest_pros = (best_w1 * lgb_test_pro + best_w2 * cat_test_pro + (steps - best_w1 - best_w2) * xgb_test_pro) / steps\ntest['target'] = test_pros\nprint(test_pros[:10])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submission","metadata":{}},{"cell_type":"code","source":"submission=test[['isic_id','target']]\nsubmission.to_csv(\"submission.csv\",index=None)\nsubmission.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}